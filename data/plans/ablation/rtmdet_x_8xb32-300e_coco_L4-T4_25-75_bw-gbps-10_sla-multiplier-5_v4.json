[
  {
    "xput": 1951.8913492070026,
    "config": {
      "dnn_name": "rtmdet_x_8xb32-300e_coco",
      "gpu_name_arr": [
        "L4",
        "T4"
      ],
      "sla": 117736.42093023255,
      "num_mps_levels": [
        4,
        4
      ],
      "max_num_parts": 3,
      "bs_same": true,
      "transmit_time_us_arr": [
        6553.599999999999,
        6553.599999999999,
        3276.7999999999997,
        3276.7999999999997,
        5734.4,
        4505.6,
        5734.4,
        5120.0,
        1128.96
      ],
      "est_xput": 1611.7606364880653,
      "batch_build_factor": 1.3,
      "hist_adjustment": 11774,
      "hist_adjustment_w_scheduling": 0,
      "num_gpu_per_server_arr": [
        4,
        2
      ],
      "force_sum_gpu_integer_per_partition": true,
      "bw_gbps": 10,
      "runtime_fmt": 1
    },
    "sla": 117736.42093023255,
    "pipelines": [
      {
        "xput": 10.867439017127893,
        "partitions": [
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              0,
              10
            ],
            "gpu": "T4",
            "mps": 0,
            "bs": 1,
            "num_gpu_per_server": 2,
            "num_gpu": 1.0,
            "lat_infer": 92018.0,
            "lat_trans": 0.0,
            "xput": 10.867438979330132
          }
        ],
        "est_xput": 1611.7606364880653,
        "est_batch_build_lat": 0.0,
        "batch_build_factor": 1.3,
        "hist_adjustment": 11774,
        "hist_adjustment_w_scheduling": 0
      },
      {
        "xput": 1906.3572695577336,
        "partitions": [
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              0,
              4
            ],
            "gpu": "T4",
            "mps": 1,
            "bs": 1,
            "num_gpu_per_server": 2,
            "num_gpu": 37.5,
            "lat_infer": 39326.0,
            "lat_trans": 3276.7999999999997,
            "xput": 1907.1352286019428
          },
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              4,
              9
            ],
            "gpu": "L4",
            "mps": 1,
            "bs": 1,
            "num_gpu_per_server": 4,
            "num_gpu": 25.0,
            "lat_infer": 26123.0,
            "lat_trans": 1128.96,
            "xput": 1907.3486328125
          },
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              9,
              10
            ],
            "gpu": "T4",
            "mps": 1,
            "bs": 1,
            "num_gpu_per_server": 2,
            "num_gpu": 32.999997514679116,
            "lat_infer": 34621.0,
            "lat_trans": 0.0,
            "xput": 1906.3572695577318
          }
        ],
        "est_xput": 1611.7606364880653,
        "est_batch_build_lat": 0.0,
        "batch_build_factor": 1.3,
        "hist_adjustment": 11774,
        "hist_adjustment_w_scheduling": 0
      },
      {
        "xput": 34.66645098559319,
        "partitions": [
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              0,
              1
            ],
            "gpu": "T4",
            "mps": 1,
            "bs": 1,
            "num_gpu_per_server": 2,
            "num_gpu": 0.5,
            "lat_infer": 11289.0,
            "lat_trans": 6553.599999999999,
            "xput": 38.14697265625
          },
          {
            "dnn": "rtmdet_x_8xb32-300e_coco",
            "layers": [
              1,
              10
            ],
            "gpu": "T4",
            "mps": 0,
            "bs": 1,
            "num_gpu_per_server": 2,
            "num_gpu": 3.0,
            "lat_infer": 86539.0,
            "lat_trans": 0.0,
            "xput": 34.66645096430511
          }
        ],
        "est_xput": 1611.7606364880653,
        "est_batch_build_lat": 0.0,
        "batch_build_factor": 1.3,
        "hist_adjustment": 11774,
        "hist_adjustment_w_scheduling": 0
      }
    ],
    "mipgap": 0.31992854284206657,
    "runtime": 1800.2028946876526
  }
]